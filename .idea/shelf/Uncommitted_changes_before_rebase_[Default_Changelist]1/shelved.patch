Index: Layers/MaxPooling.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\nfrom typing import Tuple, Generator\r\nfrom Layers.Layer import Layer\r\n\r\n\r\nclass MaxPooling(Layer):\r\n\r\n    def __init__(self, pool_size: int = 2, stride: int = 2) -> None:\r\n        \"\"\"\r\n\r\n        :param pool_size: (Depth, Height, Width) of input\r\n        :param stride:\r\n        \"\"\"\r\n        self.pool_size = pool_size\r\n        self.stride = stride\r\n\r\n        self.input = None\r\n        self.output = None\r\n\r\n    def iterate_regions(self, image: np.array) -> Generator:\r\n        \"\"\"\r\n        Generates all possible 3*3 sub-images from image\r\n        using valid padding (no padding, drops extra values)\r\n        :param image: Pixels matrix of image\r\n        :return: Generator of all sub-images\r\n        \"\"\"\r\n        c_in, h_in, w_in = image.shape\r\n\r\n        # Compute output shape\r\n        h_out = 1 + (h_in - self.pool_size) // self.stride\r\n        w_out = 1 + (w_in - self.pool_size) // self.stride\r\n\r\n        # Loop through all values in the image\r\n        # Moving the kernel Horizontally and then Vertically\r\n        for y in range(h_out):\r\n            for x in range(w_out):\r\n                # Prepare for slicing image\r\n                top = y * self.stride\r\n                bottom = y * self.stride + self.pool_size\r\n                left = x * self.stride\r\n                right = x * self.stride + self.pool_size\r\n\r\n                image_slice = image[:, top:bottom, left:right]\r\n                yield image_slice, y, x\r\n\r\n    def forward(self, image: np.array):\r\n        \"\"\"\r\n        :param inputs: output of previous layer (depth, width, height)\r\n        :return: Current layer outputs\r\n        \"\"\"\r\n\r\n        self.input = image\r\n\r\n        c_in, h_in, w_in = image.shape\r\n\r\n        # Compute output shape\r\n        h_out = 1 + (h_in - self.pool_size) // self.stride\r\n        w_out = 1 + (w_in - self.pool_size) // self.stride\r\n\r\n        self.output = np.zeros((c_in, h_out, w_out))\r\n\r\n        # Loop though output matrix\r\n        for c in range(c_in):\r\n            for image_slice, y, x in self.iterate_regions(image):\r\n                # Get maximum value in image slice\r\n                self.output[c, y, x] = np.max(image_slice[c])\r\n\r\n        return self.output\r\n\r\n    def backward(self, loss_gradient: np.array, lr: int) -> np.array:\r\n        \"\"\"\r\n        Backpropagation in a max-pooling layer\r\n        :return: the derivative of the cost layer with respect to the current layer\r\n        \"\"\"\r\n\r\n        c_in, h_in, w_in = self.input.shape\r\n\r\n\r\n        loss_gradient_out = np.zeros(self.input.shape)\r\n\r\n        for c in range(c_in):\r\n            for image_slice, y, x in self.iterate_regions(self.input):\r\n                # indexes of max value image slice\r\n                print(np.unravel_index(np.nanargmax(image_slice), image_slice.shape))\r\n                (idy, idx, _) = np.unravel_index(np.nanargmax(image_slice), image_slice.shape)\r\n\r\n                loss_gradient_out[c, y * self.stride + idy, x * self.stride + idx] += loss_gradient[c, y, x]\r\n        print(\"DONE\\n\\n\\n\\n\\n\\n\\n\")\r\n        return loss_gradient_out\r\n\r\n
===================================================================
--- Layers/MaxPooling.py	(revision ac59d39392425a261ca3fb43bf8a4fac76f906ba)
+++ Layers/MaxPooling.py	(date 1644786790458)
@@ -63,7 +63,7 @@
         for c in range(c_in):
             for image_slice, y, x in self.iterate_regions(image):
                 # Get maximum value in image slice
-                self.output[c, y, x] = np.max(image_slice[c])
+                self.output[c, y, x] = np.nanmax(image_slice[c])
 
         return self.output
 
@@ -81,10 +81,9 @@
         for c in range(c_in):
             for image_slice, y, x in self.iterate_regions(self.input):
                 # indexes of max value image slice
-                print(np.unravel_index(np.nanargmax(image_slice), image_slice.shape))
                 (idy, idx, _) = np.unravel_index(np.nanargmax(image_slice), image_slice.shape)
 
                 loss_gradient_out[c, y * self.stride + idy, x * self.stride + idx] += loss_gradient[c, y, x]
-        print("DONE\n\n\n\n\n\n\n")
+
         return loss_gradient_out
 
Index: Layers/Convolutional.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\nfrom typing import Tuple\r\nfrom scipy import signal\r\nfrom Layers.Layer import Layer\r\n\r\nclass Convolutional(Layer):\r\n    def __init__(self, input_shape: Tuple[int, int, int], kernel_size: int, depth: int):\r\n        \"\"\"\r\n\r\n        :param input_shape: (Height, Width, Channels)\r\n        :param kernel_size: Width, Height of kernel\r\n        :param depth: How many kernels do we want? -> depth of the output layer\r\n        \"\"\"\r\n        # unpack input shape\r\n        channels, input_height, input_width = input_shape\r\n        self.input_shape = input_shape\r\n\r\n        self.depth = depth\r\n        self.channels = channels\r\n\r\n        # Compute output shape\r\n        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\r\n\r\n        # Compute kernel shape, 4d because we have multiple kernels and each kernel is a 3d block\r\n        self.kernels_shape = (depth, channels, kernel_size, kernel_size)\r\n\r\n        # Initialize kernels and biases\r\n        # biases have the same shape of the output\r\n        self.kernels = np.random.randn(*self.kernels_shape) * 0.1\r\n        self.biases = np.random.randn(*self.output_shape) * 0.1\r\n\r\n        self.inputs = None\r\n        self.output = None\r\n\r\n    def forwards(self, inputs):\r\n\r\n        self.inputs = inputs\r\n\r\n        # Add biases to output\r\n        self.output = np.copy(self.biases)\r\n\r\n        for n in range(self.depth):\r\n            for c in range(self.channels):\r\n                # Calculates the \"convolution\" between image and kernels\r\n                self.output[n] += signal.correlate2d(inputs[c], self.kernels[n, c], \"valid\")\r\n\r\n        return self.output\r\n\r\n\r\n    def backwards(self, output_gradient, lr):\r\n\r\n        # initialize gradients\r\n        kernels_gradient = np.zeros(self.kernels_shape)\r\n        input_gradient = np.zeros(self.input_shape)\r\n\r\n        for n in range(self.depth):\r\n            for c in range(self.channels):\r\n                kernels_gradient[n, c] = signal.correlate2d(self.inputs[c], output_gradient[n], \"valid\")\r\n                input_gradient[c] += signal.convolve2d(output_gradient[n], self.kernels[n, c], \"full\")\r\n\r\n        self.kernels -= lr * kernels_gradient\r\n        self.biases -= lr * output_gradient\r\n\r\n        return input_gradient\r\n
===================================================================
--- Layers/Convolutional.py	(revision ac59d39392425a261ca3fb43bf8a4fac76f906ba)
+++ Layers/Convolutional.py	(date 1644849354620)
@@ -1,64 +1,90 @@
 import numpy as np
-from typing import Tuple
+from typing import Tuple, Generator
 from scipy import signal
 from Layers.Layer import Layer
 
 class Convolutional(Layer):
-    def __init__(self, input_shape: Tuple[int, int, int], kernel_size: int, depth: int):
-        """
 
-        :param input_shape: (Height, Width, Channels)
-        :param kernel_size: Width, Height of kernel
-        :param depth: How many kernels do we want? -> depth of the output layer
+    def __init__(self, num_filters: int = 16, kernel_size: int = 3, stride: int = 1, activation: str = None) -> None:
+        """
+        A Convolution layer using a 3x3 kernel
+        :param depth: Number of filters
         """
-        # unpack input shape
-        channels, input_height, input_width = input_shape
-        self.input_shape = input_shape
+
+        self.num_filters = num_filters
+        self.kernel_size = kernel_size
+        self.stride = stride
+        self.activation = activation
+
+        # Initialize Filters: Stack of kernels
+        self.filters = np.random.randn(num_filters, kernel_size, kernel_size) * 0.1
+
+        self.input = None
+        self.output = None
+
 
-        self.depth = depth
-        self.channels = channels
+    def iterate_regions(self, image: np.array) -> Generator:
+        """
+        Generates all possible 3*3 sub-images from image
+        using valid padding (no padding, drops extra values)
+        :param image: Pixels matrix of image
+        :return: Generator of all sub-images
+        """
+        c_in, h_in, w_in = image.shape
 
         # Compute output shape
-        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)
+        h_out = 1 + (h_in - self.kernel_size) // self.stride
+        w_out = 1 + (w_in - self.kernel_size) // self.stride
+
 
-        # Compute kernel shape, 4d because we have multiple kernels and each kernel is a 3d block
-        self.kernels_shape = (depth, channels, kernel_size, kernel_size)
+        # Loop through all values in the image
+        # Moving the kernel Horizontally and then Vertically
+        for y in range(h_out):
+            for x in range(w_out):
+                # Prepare for slicing image
+                top = y * self.stride
+                bottom = y * self.stride + self.kernel_size
+                left = x * self.stride
+                right = x * self.stride + self.kernel_size
 
-        # Initialize kernels and biases
-        # biases have the same shape of the output
-        self.kernels = np.random.randn(*self.kernels_shape) * 0.1
-        self.biases = np.random.randn(*self.output_shape) * 0.1
+                image_slice = image[:, top:bottom, left:right]
+                yield image_slice, y, x
 
-        self.inputs = None
-        self.output = None
 
-    def forwards(self, inputs):
+    def forward(self, image) -> np.array:
 
-        self.inputs = inputs
+        # Keep track of input for backpropagation
+        self.input = image
 
-        # Add biases to output
-        self.output = np.copy(self.biases)
+        _, h, w = image.shape
 
-        for n in range(self.depth):
-            for c in range(self.channels):
-                # Calculates the "convolution" between image and kernels
-                self.output[n] += signal.correlate2d(inputs[c], self.kernels[n, c], "valid")
+        # Compute output shape
+        self.output = np.zeros((self.num_filters, h - self.kernel_size + 1, w - self.kernel_size + 1))
+
+        # Convolve each filter over image
+        for f in range(self.num_filters):
+            for image_slice, y, x in self.iterate_regions(image):
+                self.output[f, y, x] += np.nansum(self.filters[f] * image_slice)
 
         return self.output
 
+    def backward(self, loss_gradient: np.array, lr: int) -> np.array:
+
+        _, h, w = self.input.shape
 
-    def backwards(self, output_gradient, lr):
+        # Loss Gradient of previous Layer
+        loss_gradient_out = np.zeros(self.input.shape)
 
-        # initialize gradients
-        kernels_gradient = np.zeros(self.kernels_shape)
-        input_gradient = np.zeros(self.input_shape)
+        # Loss Gradient of all kernels
+        filters_gradient = np.zeros(self.filters.shape)
 
-        for n in range(self.depth):
-            for c in range(self.channels):
-                kernels_gradient[n, c] = signal.correlate2d(self.inputs[c], output_gradient[n], "valid")
-                input_gradient[c] += signal.convolve2d(output_gradient[n], self.kernels[n, c], "full")
+        for f in range(self.num_filters):
 
-        self.kernels -= lr * kernels_gradient
-        self.biases -= lr * output_gradient
+            for image_slice, y, x in self.iterate_regions(self.input):
+                filters_gradient[f] += np.nansum(loss_gradient[f, y * self.stride, x * self.stride] * image_slice, axis=0)
+                loss_gradient_out[:, y * self.stride: y * self.stride + self.kernel_size,
+                                     x * self.stride: x * self.stride + self.kernel_size] += loss_gradient[f, y, x] * self.filters[f]
 
-        return input_gradient
+        self.filters -= lr * filters_gradient
+
+        return loss_gradient_out
Index: Layers/Activations.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\nfrom typing import Tuple\r\nfrom Layers.Layer import Layer\r\n\r\n\r\nclass Softmax(Layer):\r\n    def __init__(self):\r\n        self.inputs = None\r\n        self.output = None\r\n\r\n    def forward(self, inputs):\r\n        self.inputs = inputs\r\n        tmp = np.exp(inputs)\r\n        self.output = tmp / np.sum(tmp)\r\n        return self.output\r\n\r\n    def backward(self, output_gradient, lr):\r\n\r\n        output_gradient = np.empty((self.output.shape[0], -1))\r\n\r\n\r\n        for i in range(len(self.output)):\r\n            for j in range(len(self.inputs)):\r\n                if i == j:\r\n                    output_gradient[i] = self.output * (1 - self.inputs[i])\r\n                else:\r\n                    output_gradient[i] = -self.output[i]*self.inputs[i]\r\n\r\n        return output_gradient\r\n\r\nclass Sigmoid(Layer):\r\n\r\n    def __init__(self):\r\n        self.inputs = None\r\n        self.output = None\r\n\r\n    def forward(self, inputs):\r\n        self.output = 1.0 / (1.0 + np.exp(-inputs))\r\n        return self.output\r\n\r\n    def backward(self, output_gradient, lr):\r\n\r\n        inputs_gradient = output_gradient * self.output * (1 - self.output)\r\n        return inputs_gradient\r\n\r\nclass Relu(Layer):\r\n\r\n    def __init__(self):\r\n        self.inputs = None\r\n        self.output = None\r\n\r\n    def forward(self, inputs) -> np.array:\r\n        self.output= np.maximum(0, inputs)\r\n        return self.output\r\n\r\n    def backward(self, da_curr: np.array, lr) -> np.array:\r\n        dz = np.array(da_curr, copy=True)\r\n        dz[self.output <= 0] = 0\r\n        return dz\r\n
===================================================================
--- Layers/Activations.py	(revision ac59d39392425a261ca3fb43bf8a4fac76f906ba)
+++ Layers/Activations.py	(date 1644794317368)
@@ -5,28 +5,21 @@
 
 class Softmax(Layer):
     def __init__(self):
-        self.inputs = None
+        self.input = None
         self.output = None
 
     def forward(self, inputs):
-        self.inputs = inputs
-        tmp = np.exp(inputs)
-        self.output = tmp / np.sum(tmp)
+        self.input = inputs
+        self.output = np.exp(inputs) / np.nansum(np.exp(inputs))
         return self.output
 
+
+
+    def backward(self, grad):
+        return self.old_y * (grad - np.nansum(grad * self.old_y))
+
     def backward(self, output_gradient, lr):
-
-        output_gradient = np.empty((self.output.shape[0], -1))
-
-
-        for i in range(len(self.output)):
-            for j in range(len(self.inputs)):
-                if i == j:
-                    output_gradient[i] = self.output * (1 - self.inputs[i])
-                else:
-                    output_gradient[i] = -self.output[i]*self.inputs[i]
-
-        return output_gradient
+        return self.output * (output_gradient - np.nansum(output_gradient * self.output))
 
 class Sigmoid(Layer):
 
Index: Layers/Dense.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\nfrom Layers.Layer import Layer\r\n\r\nclass Dense(Layer):\r\n    def __init__(self, input_size, output_size):\r\n        self.weights = np.random.randn(output_size, input_size)\r\n        self.biases = np.random.randn(output_size, 1)\r\n\r\n\r\n    def forward(self, inputs):\r\n\r\n        self.inputs = inputs\r\n        return np.dot(self.weights, inputs) + self.biases\r\n\r\n\r\n\r\n    def backward(self, output_gradient, learning_rate):\r\n\r\n        m = self.inputs.shape[0]\r\n        weights_gradient = np.dot(output_gradient, self.inputs.T) / m\r\n        biases_gradient = np.sum(output_gradient, axis=0, keepdims=True) / m\r\n\r\n        self.weights -= learning_rate * weights_gradient\r\n        self.biases -= learning_rate * biases_gradient\r\n\r\n        input_gradient = np.dot(self.weights.T, output_gradient)\r\n        return input_gradient
===================================================================
--- Layers/Dense.py	(revision ac59d39392425a261ca3fb43bf8a4fac76f906ba)
+++ Layers/Dense.py	(date 1644786790452)
@@ -18,7 +18,7 @@
 
         m = self.inputs.shape[0]
         weights_gradient = np.dot(output_gradient, self.inputs.T) / m
-        biases_gradient = np.sum(output_gradient, axis=0, keepdims=True) / m
+        biases_gradient = np.nansum(output_gradient, axis=0, keepdims=True) / m
 
         self.weights -= learning_rate * weights_gradient
         self.biases -= learning_rate * biases_gradient
